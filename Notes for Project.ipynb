{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing\n",
    "\n",
    "Using Scikit-Image, Pillow, OpenCV to make patterns stand out more.\n",
    "\n",
    "We will now initialize the parameters of the model, perform forward propagation, and calculate the current loss. Perform backward propagation (basically calculating the current gradient), and update the parameters (with gradient descent).\n",
    "\n",
    "MLP's take their input as vectors, not matrices or tensors. If all of the images were different sizes, then we would have a more significant problem on our hands, because we'd have challenges getting each image reshaped into a vector the exact same size as our input layer.\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "## PCA for Compression\n",
    "\n",
    "After dimensionality reduction, the training set takes up less space. Applying this to our images, we can preserve 95% of the variance, but will be less than 20% of the original size. This is a reasonable compression ratio, and the size reduction can speed up a classification algorithm (such as a Support Vector Machine) considerably. The mean squared distance between the original data and the reconstructed data (compressed and then decompressed) is called the reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code compresses the xray images\n",
    "# and then decompresses back to the original dimensions.\n",
    "# There will be some image quality loss, but not so much as \n",
    "# to be unusable.\n",
    "pca = PCA(n_components=154)\n",
    "xreduced = pca.fit_transform(xtrain)\n",
    "xrecovered = pca.inverse_transform(xreduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression in a Convolutional Neural Network\n",
    "\n",
    "The purpose of a neuron in a neural network is 2-fold:\n",
    "\n",
    "1. Transforms the inputs ding a linear transformation (generally wx+b).\n",
    "2. Uses an activation function (generally sigmoid).\n",
    "\n",
    "Logistic regression is a 1-layer neural network as the output is either 0 or 1. The input layer is not a counted layer. No transformations happen at the input layer, it's just where the inputs are added. In each of the nodes of a hidden layer, a linear transformation will take place, as well as a transformation because of an activation function. \n",
    "\n",
    "Activation functions determine the output of a node from a given set of inputs. Most NN's are optimized using some form of gradient descent, activations functions need to be differentiable (or almost entirely differentiable... see ReLu).\n",
    "\n",
    "Since we have a binary classification task, the output layer should be a dense layer with a single neuron, and the activation set to 'sigmoid'.\n",
    "\n",
    "model_1.add(Dense(1, activation='sigmoid'))\n",
    "model_1.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "results_1 = model_1.fit(scaled_data, labels, epochs=25, batch_size=1, validation_split=0.2)\n",
    "\n",
    "## The Hyperbolic Tangent (tanh) function\n",
    "\n",
    "The hyperbolic tangent function goes between -1 and 1 and is a shifted version of the sigmoid function. For intermediate layers, the tanh function generally performs pretty well because, with values between -1 and 1, the means of the activations coming out are closer to zero.\n",
    "\n",
    "The disadvantage of both tanh and sigmoid activation functions is that when z gets quite large or small, the derivative of the slopes of these functions become very small, generally 0.0001. This will slow down gradient descent.\n",
    "\n",
    "## The Inverse Tangent (arctan) function\n",
    "\n",
    "The inverse tangent has a lot of the same qualities that tanh has, but the range roughly goes from -1.6 to 1.6 and the slope is more gentle than the one we saw with tanh.\n",
    "\n",
    "## Rectified Linear Unit function\n",
    "\n",
    "This is probably the most popular activation function, along with tanh. The fact that the activation is exaclty 0 when z < 0 is slightly cumbersome when taking derivatives, though.\n",
    "\n",
    "## The Leaky Rectified Linear Unit function\n",
    "\n",
    "The leaky ReLu solves the derivatives issue by allowing for the activation ot be slightly negative when z < 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "Obtaining the expression for the cost function and the loss function is called forward propagation. The cost function takes a convex form. The idea is that we start with some initial values of w abd b and then graident descent takes a step in the steepest direction downhill. Taking the derivatives to calculate the difference between the desired and calcualted outcome, repeating these steps until you get the lowest possible cost value, is called backpropagation.\n",
    "\n",
    "Neural Networks work like this: We perform the dot product for input data set with the weight set. We feed z into an activation function, f(z) so that we get an output for the hidden layer's first neuron. F() need not be a step function. We repeat this with the same input data and weights to the second hidden neuron, and then we will get the output for the second hidden neuron in another hidden layer. We repeat these steps until the last weight using input data from the nth instance. Now the hidden layer outputs are calcualted, and used as inputs to calculate the final output. For the final output, perform a dot product for hidden layer outputs, which we now consider as inputs, and hidden layer weights. Remember that the inputs are hidden layer outputs. Since we only have 1 output, our set of weights consists of a range with n weights, since we have n hidden layer inputs. Then we add the sum of the dot product with bias to obtain a result z. We feed z into an activation function f(z) so that we get an output for the output layer.\n",
    "\n",
    "If the activation function is linar, then you can stack as many hidden layers in the neural network as you wish, and the final output is still a linear combination of the original input data. Here, a small change in any weight in the input layer of our perceptron network could possibly lead to one neuron to suddenly flip from 0 to 1, which could again affect the hidden layer's behavior and then affect the final outcome; with a step function as an activation function. The sigmoid neuron, which comes with the sigmoid function is another activation function. The sigmoid function produces similar results to step function in that the output is between 0 and 1; the curve is smooth, and has a nice and simple derivative, which is differentiable everywhere on the curve. Sigmoid introduces non-linearity into our neural network model. The output layer of a neural node represents a combinatorial charge.\n",
    "\n",
    "Neural networks become better by repetitively training themselves on data so they can adjust each layer of the network to get the final results/actual output closer to the desired output. So when we actually train this neural network with al the training examples, we don't know what weights we should assign to each of the layers. So we just randomly ask the computer to assign weights in each layer.\n",
    "\n",
    "The concept of randomly initializing weights is important because each time you train a deep learning neural network, you are initializing different numbers to the weights. So, essentially, you are initializing different numbers to the weights and you have no clue what's going on in the network until after the network is trained. A trained neural network has weights which are optimized at certain values that make the best prediction or classification of our problem. It's a black box, and each time the network will have different sets o weights.\n",
    "\n",
    "In multi layer neural networks, the first hidden layer will be able to learn some very simple patterns. Each additional hidden layer will somehow be able to learn progressively more complicated patterns.\n",
    "\n",
    "In forward propagation, you get x and you compute y_hat; after, you calculate the cost function.\n",
    "\n",
    "Here is how forward propagation is performed/calculated: Z1 is the output of the linear transformation of the initial input, A1 (the observations). In successive layers, A1 is the output from the previous hidden layer. In all of these cases, W1 is a matrix of weights to be optimized to minimize the cost function. B1 is also optimized but is a vector as opposed to a matrix.\n",
    "\n",
    "G1 is the activation function which takes the outut of this linear transformation and yields the input to the next hidden layer.\n",
    "\n",
    "## Backward Propagation\n",
    "\n",
    "Once an output for the NN given the current parameter weights has been calculated, we must back propagate to calculate the gradients of layer paramters with respect to the cost function. This will allow us to apply an optimization algorithm such as gradient descent in order to make small adjustments to the parameters in order to minimize our cost (and improve our predictions).\n",
    "\n",
    "To summarize the process once more, we begin by defining a model architecture which includes the number of hidden layers, activation functions, and the number of units in each of these. When we initialize parameters for each of these layers (typically randomly). After the initial parameters are set, forward propagation evaluates the model given a prediction, which is then used to evaluate a cost function. Forward propagation involves evaluating each layer and then piping this output into the next layer.\n",
    "\n",
    "Each layer consists of a linear transformation and an activation function. The paramters for the linear transformation in each layer include W1 and B1. The output of this linear transfromation is represented by Z1. This is then fed through the activation function (again, for each layer) giving us an output A which is the input for the next layer of the model.\n",
    "\n",
    "After forward propagation is completed and the cost function is evaluated, back propagation is used to calculate gradients of the initial paramters with respect to this cost function. Finally, these gradients are then used to optimize the algorithm, such as gradient descent, to make small adjustments to the parameters and the entire process of forward propagation, back propagation, and parameter adjustments is repeated until the modeller is satisfied with the results.\n",
    "\n",
    "We use batches because if we were to push all of our samples through at once, we would have to wait until everyhthing is processed and can only start backpropagating then. Therefore, batches are used, so that after each batch has done a forward propagation step, backward prop can happen again. In essence, it's 'mini-batch' gradient descent. A batch generally approximates the distribution of the input data better than a single output. The larger the batch, the better the approximation. However, it is also true that the batch will take longer to process and will result in only one update. For inference (to evaluate, or predict) it is recommended to pick a batch size that is as large as you can afford without going out of memory. Epochs are an arbitrary cutoff, generally defined as one pass over the entire dataset, used to separate training into distinct phases, which is useful for logging and periodic evaluation. When using valudation_data or validation_split, with the .fit() method of Keras, evaluation will be run at the end of every epoch. Within Keras, there is the ability to add callbacks specifically designed to be run at the end of the epoch. Examples of these are learning rate changes and model checkpointing (saving).\n",
    "\n",
    "Samples in a neural network are one element of the dataset. ex. an image.\n",
    "\n",
    "history.history will return a dictionary of the metrics we indicated when compiling the model. By default, the loss criteria will always be included as well. This dictionary will have two keys, one for the loss and one for the accuracy. If you want to plot learning curves for the loss or accuracy versus the epochs, you can then simply retrieve these lists.\n",
    "\n",
    "Making predictions from a trained model is straightforward. y_hat = model.predict(x).\n",
    "\n",
    "We use the .evaluate() method in order to compute the loss and other specified metrics for our train model. model.evaluate(xtrain, xtrain_labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we have initialized a network object, we can then add layers\n",
    "# which inlcudes the number of layers we wish to add, as well as the \n",
    "# activation function we want to use. We can use sigmoid, ReLu, etc.\n",
    "# The Dense() class indicates that this layer will be fully connected\n",
    "# The input_shape parameter is optional. In successive layers, Keras\n",
    "# implies the required shape of the layer to be added based on the shape\n",
    "# of the previous layer.\n",
    "# model.add(layers.Dense(units, activation, input_shape))\n",
    "\n",
    "# Train the model\n",
    "# history = model.fit(xtrain,\n",
    "                    # ytrain, epochs=20,\n",
    "                    # batch_size = 512, \n",
    "                    # validation_data = (xval, yval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can detect overfitting if the model's training performance steadily improves long after the validation performance plateaus. By adding another hidden layer, we can give the model the ability to capture more high-level abstraction in the data. However, increasing the depth of the model also increases the amount of data the model needs to converge to answer, because a more complex model comes with the 'Curse of Dimensionality'. \n",
    "\n",
    "Even with a good validation score, a model is not acceptable if the training and validation accuracies do not converge. We can remedy this by decreasing the size of the network, or by increasing the size of our training data.\n",
    "\n",
    "Deep representations are really good at automating what used to be a tedious process of feature engineering. For example, deep layers of a neural network for a computer might look like this: \n",
    "\n",
    "1. First layer detects edges in pictures.\n",
    "2. Second layer groups edges together and starts to detect different parts.\n",
    "3. More layers, group even bigger parts together.\n",
    "\n",
    "The general idea is shallow networks detect \"simple\" things and the deeper you go, the more complex things that can be detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "\n",
    "Scalar is a 0D tensor, Vectors are 1D tensors, Matrices are 2D tensors, and then there are 3D tensors. A tensor is defined by three key attributes: the rank or number of axes, the shape, the data type.\n",
    "\n",
    "Unrowing matrices is important for images. Then we increase the rank. Ex.) vector with np.shape() (790,) -> np.reshape(vector, (1,790)).\n",
    "\n",
    "We can slice tensors using the the usual syntax of start index : end index.\n",
    "\n",
    "The dot product is the sum of the element wise products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters with Regularization\n",
    "\n",
    "There are many hyperparameters you can tune. These include: \n",
    "\n",
    "1. number of hidden units\n",
    "2. number of layers\n",
    "3. learning rate (alpha)\n",
    "4. activation function\n",
    "\n",
    "The question then becomes how do you choose these paramters? One primary method is to develop validation sets to strike a balance between specificity and generalization.\n",
    "\n",
    "When tuning neural networks, it typically helps to split the data into three distinct partitions as follows:\n",
    "\n",
    "1. Train algorithms on the training set.\n",
    "2. Use a validation set to decide which one will be your final model after paramter tuning.\n",
    "3. After having chosen the final model, and havng evaluated long enough, you'll use the test set to get an unbiased estimate of the classification performance (or whatever your evaluation metric will be).\n",
    "\n",
    "Remember that it is very important to make sure that the holdout validation set and the test samples come from the same distribution ex. the same resolution of santa pictures.\n",
    "\n",
    "Bias = underfitting\n",
    "High variance = overfitting\n",
    "Good fit = somewhere in between.\n",
    "\n",
    "We use regularization when the model overfits to the data. L1 and L2 regularization.\n",
    "\n",
    "## Dropout Regularization\n",
    "\n",
    "When you apply the Dropout technique, a random subset of nodes (also called the units) in a layer are ignored (their weights are set to zero) during each phase of training; allows us to train neural networks on different parts of the data, thus ensuring that our model is not overly sensitive to noise in the data.\n",
    "\n",
    "In Keras, you specifically Dropout using the Dropout layer, which is applied to input and hidden layers. The Dropout layer requires one argument, rate, which specifices the fraction of units to drop, usually between 0.2 and 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(5, activation='relu', input_shape=(500,)))\n",
    "# Dropout applied to the input layer\n",
    "model.add(layers.Drouput(0.3))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "# Dropout applied to the hidden layer\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# In different iterations through the training set, different nodes\n",
    "# will be zeroed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample the data to test how model reacts to different sizes\n",
    "df_sample = df.sample(10000, random_state=123)\n",
    "# Split the data into x an y\n",
    "y = df_sample['Product']\n",
    "x = df_sample['Consumer complaint narrative']\n",
    "# Train Test Split\n",
    "xtrain, x test, ytrain, ytest = train_test_split(x, y, test_size=1500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set\n",
    "xtrain_final, xval, ytrain_final, yval = train_test_split(xtrain, ytrain, test_size=1000, random_state=42\n",
    "# Good practice to set aside a validation set, which is then used during\n",
    "# Hyperparameter tuning. Afterwards, the test set can be used to determine\n",
    "# the unbiased performance of the model.                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "Overfitting-- it's not possible to know in advance how many epochs you need to train your model on, and running the model multiple times with varying number of epochs may be helpful, but it is a time consuming process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import EarlyStopping and ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define the callbacks\n",
    "early_stopping = [EarlyStopping(monitor='val_loss', patience=10), \n",
    "                  ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_val = model_2.fit(X_train_tokens, \n",
    "                          y_train_lb, \n",
    "                          epochs=150, \n",
    "                          callbacks=early_stopping, \n",
    "                          batch_size=256, \n",
    "                          validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regularizers\n",
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "L2_model = models.Sequential()\n",
    "\n",
    "# Add the input and first hidden layer\n",
    "L2_model.add(layers.Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,)))\n",
    "\n",
    "# Add another hidden layer\n",
    "L2_model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "\n",
    "# Add an output layer\n",
    "L2_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "L2_model.compile(optimizer='SGD', \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Train the model \n",
    "L2_model_val = L2_model.fit(X_train_tokens, \n",
    "                            y_train_lb, \n",
    "                            epochs=150, \n",
    "                            batch_size=256, \n",
    "                            validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 model details\n",
    "L2_model_dict = L2_model_val.history\n",
    "L2_acc_values = L2_model_dict['acc'] \n",
    "L2_val_acc_values = L2_model_dict['val_acc']\n",
    "\n",
    "# Baseline model\n",
    "baseline_model_acc = baseline_model_val_dict['acc'] \n",
    "baseline_model_val_acc = baseline_model_val_dict['val_acc']\n",
    "\n",
    "# Plot the accuracy for these models\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "ax.plot(epochs, L2_acc_values, label='Training acc (L2)')\n",
    "ax.plot(epochs, L2_val_acc_values, label='Validation acc (L2)')\n",
    "ax.plot(epochs, baseline_model_acc, label='Training acc (Baseline)')\n",
    "ax.plot(epochs, baseline_model_val_acc, label='Validation acc (Baseline)')\n",
    "ax.set_title('Training & validation accuracy L2 vs regular')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "L1_model = models.Sequential()\n",
    "\n",
    "# Add the input and first hidden layer\n",
    "L1_model.add(layers.Dense(50, activation='relu', kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,)))\n",
    "\n",
    "# Add a hidden layer\n",
    "L1_model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "\n",
    "# Add an output layer\n",
    "L1_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "L1_model.compile(optimizer='SGD', \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Train the model \n",
    "L1_model_val = L1_model.fit(X_train_tokens, \n",
    "                            y_train_lb, \n",
    "                            epochs=150, \n",
    "                            batch_size=256, \n",
    "                            validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "L1_model_dict = L1_model_val.history\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "ax.plot(epochs, acc_values, label='Training acc L1')\n",
    "ax.plot(epochs, val_acc_values, label='Validation acc L1')\n",
    "ax.set_title('Training & validation accuracy with L1 regularization')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚è∞ This cell may take about a minute to run\n",
    "random.seed(123)\n",
    "dropout_model = models.Sequential()\n",
    "\n",
    "# Implement dropout to the input layer\n",
    "# NOTE: This is where you define the number of units in the input layer\n",
    "dropout_model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "\n",
    "# Add the first hidden layer\n",
    "dropout_model.add(layers.Dense(50, activation='relu'))\n",
    "\n",
    "# Implement dropout to the first hidden layer \n",
    "dropout_model.add(layers.Dropout(0.3))\n",
    "\n",
    "# Add the second hidden layer\n",
    "dropout_model.add(layers.Dense(25, activation='relu'))\n",
    "\n",
    "# Implement dropout to the second hidden layer \n",
    "dropout_model.add(layers.Dropout(0.3))\n",
    "\n",
    "# Add the output layer\n",
    "dropout_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "dropout_model.compile(optimizer='SGD', \n",
    "                      loss='categorical_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "dropout_model_val = dropout_model.fit(X_train_tokens, \n",
    "                                      y_train_lb, \n",
    "                                      epochs=150, \n",
    "                                      batch_size=256, \n",
    "                                      validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "results_train = dropout_model.evaluate(X_train_tokens, y_train_lb)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = dropout_model.evaluate(X_test_tokens, y_test_lb)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Neural Networks with Normalization\n",
    "\n",
    "One way to speed up training of your neural networks is to normalize the input. In fact, even if training time were not a concern, normalization to a consistent scale (0 to 1) across features should be used to ensure that the process converges to a stable solution.\n",
    "\n",
    "Not only will normalizing your inputs speed up training, it can also mitigate other risks inherent in training neural networks. Ex. having input of various ranges can lead to difficult numerical problems when the algorithm goes to compute gradients during forward and back propagation. This can lead to untenable solutions and will prevent the algorithm from converging to a solution. Activation will explode when there are many layers in the network.\n",
    "\n",
    "Aside from normalizing the data, you can also investigate the impact of changing the initialization parameters when you first launch the gradient descent algorithm.\n",
    "\n",
    "In addition, you could even use an alternative convergence algorithm instead of gradient descent. One issue with gradient descent is that it oscillates to a fairly big extent, because the derivative is bigger in the vertical direction.\n",
    "\n",
    "## Gradient Decent with Momentum\n",
    "\n",
    "Compute an exponentially weighted average of the gradients and use that gradient instead. The intuitive interpretation is that this will successively dampen oscillations improving convergence. Generally, B = 0.9 is a good hyperparameter value.\n",
    "\n",
    "## RMSProp\n",
    "\n",
    "RMSProp stands for 'root mean square' prop. It slows down learning in one direction and speeds up in another one. On each iteration, it uses exponentially weighted average of the squares of the derivatives. In the direction where we want to learn fast, the corresponding S will be small, so dividing by a small number. On the other hand, in the direction where we will want to learn slow, the corresponding S will be relatively large, and updates will be smaller. Often, add small e in the denominator to make sure that you don't end up dividing by 0.\n",
    "\n",
    "## Adam Optimization Algorithm\n",
    "\n",
    "\"Adaptive Moment Estimator,' basically using the first and second moment estimators. It takes momentum and RMSprop to put it together. Generally, only alpha gets tuned.\n",
    "\n",
    "## Learning Rate Decay\n",
    "\n",
    "Learnign rate decreases across epochs.\n",
    "\n",
    "alpha = 1/1+decay_rate * epoch_nb * alpha0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If all the values for traning and validation loss are 'nan'. This indicates that the algorithm did not converge. The first solution to this is to normalize the input. From there, if convergence is not achieved, normalizing the output may also be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "224, 251, 297, 445...... 326 hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(train_images))\n",
    "print(np.shape(train_labels))\n",
    "print(np.shape(val_images))\n",
    "print(np.shape(val_labels))\n",
    "print(np.shape(test_images))\n",
    "print(np.shape(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unrowing the data\n",
    "# Reshape the train images\n",
    "train_img_unrow = train_images.reshape(5216, -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the shape of train_img_unrow\n",
    "np.shape(train_img_unrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the val images\n",
    "val_img_unrow = val_images.reshape(16, -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the shape of val_img_unrow\n",
    "np.shape(val_img_unrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the test images\n",
    "test_img_unrow = test_images.reshape(624, -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the shape of test_img_unrow\n",
    "np.shape(test_img_unrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confirming classes\n",
    "# Confirm that all sets contain the same classes\n",
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator.class_indices\n",
    "test_generator.class_indices\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reshaping labels\n",
    "# Reshape labels for every set\n",
    "train_labels_final = train_labels.T[[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(train_labels_final)\n",
    "val_labels_final = val_labels.T[[1]]\n",
    "np.shape(val_labels_final)\n",
    "test_labels_final = test_labels.T[[1]]\n",
    "np.shape(test_labels_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensuring class identification\n",
    "# Quality Assurance -- Make sure classes are identified correctly.\n",
    "array_to_img(train_images[10])\n",
    "train_labels_final[:10]\n",
    "\n",
    "## Normalizing the data\n",
    "print(val_labels_final[:10])\n",
    "\n",
    "# Each RGB pixel in an image takes a value between 0 and 255. We need\n",
    "# to Standardize our data by dividing by 255. This will make sure each\n",
    "# pixel value is between 0 and 1.\n",
    "train_img_final = train_img_unrow/255\n",
    "val_img_final = val_img_unrow/255\n",
    "test_img_final = test_img_unrow/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baseline Sequential Model\n",
    "# Build a baseline neural network model using Keras\n",
    "baseline_model = models.Sequential()\n",
    "baseline_model.add(layers.Dense(50, activation='relu'))\n",
    "baseline_model.add(layers.Dense(25, activation='relu'))\n",
    "baseline_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# Compile the model\n",
    "baseline_model.compile(optimizer='adam', \n",
    "                       loss='binary_crossentropy', \n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "baseline_model_val = baseline_model.fit(train_generator,\n",
    "                                        epochs=5, \n",
    "                                        validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The attribute .history stored as a dictionnary now contains four\n",
    "# entries; one per metric that was being monitored during training\n",
    "# and validation. Print the keys of the dictioanry for confirmation.\n",
    "# Access the history attribute and store the dictionary\n",
    "baseline_model_val_dict = baseline_model_val.history\n",
    "\n",
    "# Print the keys\n",
    "baseline_model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate this model on the training data\n",
    "results_train = baseline_model.evaluate(X_train_tokens, y_train_lb)\n",
    "print('----------')\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test data\n",
    "results_test = baseline_model.evaluate(X_test_tokens, y_test_lb)\n",
    "print('----------')\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "loss_values = baseline_model_val_dict['loss']\n",
    "val_loss_values = baseline_model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "ax.plot(epochs, loss_values, label='Training loss')\n",
    "ax.plot(epochs, val_loss_values, label='Validation loss')\n",
    "\n",
    "ax.set_title('Training & validation loss')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot again, comparing the training and validation accuracy\n",
    "# to the number of epochs\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "acc_values = baseline_model_val_dict['acc'] \n",
    "val_acc_values = baseline_model_val_dict['val_acc']\n",
    "\n",
    "ax.plot(epochs, acc_values, label='Training acc')\n",
    "ax.plot(epochs, val_acc_values, label='Validation acc')\n",
    "ax.set_title('Training & validation accuracy')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model = Sequential([\n",
    "        Dense(32),\n",
    "        Activation('relu'),\n",
    "        Dense(10),\n",
    "        Activation('softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training a model, we need to configure the learning process, which is with the compile method. It receives three arguments: An optimizer, a loss function, a list of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization for a binary classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
