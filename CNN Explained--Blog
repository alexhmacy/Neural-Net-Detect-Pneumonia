The Pneumonia Classifier is an example of a Multi Layer Perceptron. MLPs are made up of just one input layer, and one or more hidden layers. The layers close to the input layer are usually called the lower layers and the ones close to the outputs are called the upper layers. Every layer except for the output contains a bias term, fully connected to the other layers. The signal in an MLP, or Neural Network, flows only in one direction, from the inputs to the outputs. This is known as feedforward. But with backpropagation, a network is able to compute the gradients of the network's error in relation to every model parameter; it can determine how each weight and bias term should be altered to reduce gradient error. After this, it calculates Gradient Descent and the process repeats until the network converges to a solution. One mini-batch is handled at a time, the size typically determined by the user. Every time a mini batch is processed, it is called an epoch. During these epochs, the algorithm computes the output of all the neurons in the layer. The result is passed on to the next layer, all the way down to the output layer. Afterwards, the algorithm measures the networks output error using a loss function that compares the desired output to the actual output; a measure of this error is then returned. At this point, the network computes which output contributed most to the error, using the chain rule. Then the output error of each connection below is also processed, working all the way back until the model reaches the input layer once again. Finally, the algorithm performs a Gradient Descent step to tweak all of the connection weights in the network using the error gradients it just computed. Put simply, the forward pass makes predictions, and then measures errors; backpropagation then goes through each layer to determine which weight contributed the most tothese layers, and then tweaks these layers with gradient descent. For the Pneumonia Classifier, we only need a single output neuron using the sigmoid acivation function. This gives us a number between 0 and 1, representing our classes. To achieve this, we can use Keras-- a deep learning API. First, we load our dataset. The Pneumonia Classifier contains images labeled 'Normal' or 'Pneumonia'. Each of these images is in greyscale X-Ray. After we have loaded our data, we can build a naive model using Sequential. Sequential is the simplest Keras model for Neural Networks, since it is composed of a single stack of layers connected in order. After specifying the input shape, I chose to use a Flatten layer, to convert each input image into a 1 dimensional array. Afterwards, I added a Dense layer with an activation of ReLu. Dense layers manage a weight matrix, containing all the connection weights between neurons and their outputs. Dense layers also contain a matrix of bias terms. We can see these layers by calling the model's summary. Dense layers will have a lot of parameters, since they are all of the connection weights, plus the bias terms. We compile the model by selecting an optimizer-- I chose Adam-- and then our loss metric, and our overall measure of accuracy. Fitting the model will then run each epoch, and determine the model's ability to correctly identify classes. After this, we can use the predict method on our test set to determine how well our model could predict actual classes on our target data. Depending on our results, we can add more layers to our model, or perform more data augmentation on our images. 
